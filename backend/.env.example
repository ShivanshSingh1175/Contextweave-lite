# ContextWeave Lite Backend Configuration
# Copy this file to .env and fill in your values

# ============================================
# LLM Provider Selection
# ============================================
# Choose one: groq, ollama, localai
LLM_PROVIDER=groq

# ============================================
# Cloud AI: Groq (Recommended for Speed)
# ============================================
# Get free API key from: https://console.groq.com
LLM_API_KEY=your-groq-api-key-here
LLM_API_BASE=https://api.groq.com/openai/v1
LLM_MODEL=llama-3.1-8b-instant

# ============================================
# Local AI: Ollama (Recommended for Privacy)
# ============================================
# No API key needed!
# 1. Install Ollama from: https://ollama.ai
# 2. Start server: ollama serve
# 3. Pull model: ollama pull llama3
# 4. Set LLM_PROVIDER=ollama above
# 
# Ollama runs on: http://localhost:11434
# Recommended models: llama3, mistral, codellama

# ============================================
# Local AI: LocalAI (Alternative)
# ============================================
# No API key needed!
# 1. Run: docker run -p 8080:8080 localai/localai
# 2. Set LLM_PROVIDER=localai above
#
# LocalAI runs on: http://localhost:8080

# ============================================
# Server Configuration
# ============================================
PORT=8000
